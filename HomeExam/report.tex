\documentclass[11pt,twoside,swedish]{article}
\usepackage{epsfig}
\usepackage{graphicx}
\usepackage{subfigure}
\usepackage{amssymb}
\usepackage{latexsym}
\usepackage{epic}
\usepackage{eepic}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{listings}
\usepackage[useregional]{datetime2}
\usepackage{fullpage}
\usepackage{wrapfig}
\usepackage{todonotes}
\usepackage{multirow,array}
\usepackage{subfiles}
\usepackage{xcolor,colortbl}


\setlength{\parskip}{0cm plus0mm minus0mm}

\newcommand*{\mtodo}[1]{\todo[color=violet!50]{Mikael: #1}}
\newcommand{\Mtodo}[1]{\todo[inline,color=violet!50]{Mikael: #1}}
\usepackage{comment}

\usepackage[natbib=true,citestyle=authoryear,backend=bibtex,useprefix]{biblatex}

\addbibresource{library.bib}


\usepackage{tikz}
\usetikzlibrary{shapes,arrows}

%\newtheorem{theorem}{Theorem}[section]
%\newtheorem{lemma}[theorem]{Lemma}
%\newtheorem{proposition}[theorem]{Proposition}
%\newtheorem{definition}[theorem]{Definition}
%\newtheorem{ex}{Example}[chapter]
%\newtheorem{thm}{Theorem}[chapter]
%\newtheorem{lem}{Lemma}[chapter]
%\newtheorem{cor}{Corollary}[chapter]
%\newtheorem{defn}{Definition}[chapter]
%\newtheorem{form}{Form}[chapter]
%\newtheorem*{Proof}{\it Proof}
%\newcommand{\pbox}{\hspace*{1cm}\parbox{12cm}}


\newcommand{\bx}{\hspace*{\fill}$\square$}

\renewcommand{\labelitemi}{$\cdot$}

%\textwidth 146 mm
%\textheight 230 mm
%\oddsidemargin 7mm \evensidemargin -1mm \topmargin -4mm

\begin{document}
\title{MSA220, Statistical Learning for Big Data\\
  Final Spring 2018}
\author{Mikael B\"o\"ors}

\clearpage
\maketitle
\thispagestyle{empty}

\newpage
\clearpage
\tableofcontents
\thispagestyle{empty}
\newpage

\setcounter{page}{1}

\section{Question 1}\label{Question 1}

\subsection{Mini 1}\label{Mini 1}

In order to evaluate the result of a clustering algorithm, many heuristics and
rules of thumb have been suggested. In this mini analysis I and my
group compared a few of these heuristics and evaluated their
performance on different clustering algorithms and data sets.

We chose to use artificial data sets for this task. The motivation for
this choice was that in order to evaluate the performance of the
clustering evaluation methods, we needed to know how many clusters the
data actually contained and this can be controlled when generating
artificial data. Also, the function we used to generate data allowed
us to control the shapes of the clusters, their separation, the number
of outliers and similar cluster properties. Our simulated data sets
were designed to consist of four clusters with varying spread and
numbers of observations. The generated data sets were two-dimensional
since we wanted to be able to visualize the clusters and the results
of the clustering algorithms. Also, to keep the analysis as clean as
possible we did not include any outliers or similar impurities.

The clustering algorithms we used were the EM-algorithm, K-means and
PAM (K-medoids). K-means and PAM are very similar algorithms that both
tries to choose n points in the data space that minimizes the sum of the
distances to the data points closest to them. The difference between
them is that K-means can use an arbitrary point as the cluster center,
while PAM uses observations as the medoids. Unlike K-means and PAM,
the EM algorithm is an model based clustering algorithm that tries to
find model parameters for multivariate normal distributions that
maximizes the posterior (log-) likelihood of the model. We chose to
include a model based algorithm in order to evaluate the clustering
evaluation methods on different types of algorithms.

The clustering evaluation methods we used were the average silhouette
score and the Dunn index. Both of these methods that can be applied to
the result of any clustering algorithm since they do not use the model
itself to evaluate the performance. The silhouette score for an
observation is a score that uses the average distance to all other
observations in the same cluster and the minimal average distance to
all other observations in another cluster to score how similar the
observation is to the observations in the same cluster relative to how
similar it is to the points in another cluster. The average silhouette
score is the average silhouette score for all observations. A high
average silhouette score indicates that the clusters are dense and
well separated. One can use this to determine how many clusters to
look for by choosing the number of clusters that maximizes the average
silhouette score. Similarly, the Dunn index is the ratio between the
minimal distance between observations in different clusters and the maximum
distance between observations in the same cluster. A large Dunn index
indicates dense well separated clusters and hence one can use the
index to choose the number of clusters that yields the largest Dunn
index. Since the EM algorithm is a model based algorithm, one can use
the Bayesian information criteria (BIC) to evaluate the model. The BIC
score measures how well the model fits the data while also punishing
complex models (i.e.\ it prefers simple models). A method to choose
the number of clusters is to choose the number of clusters that
minimizes the BIC score.

The experiment setup was to generate data sets containing four
clusters with different amounts of separation (-0.5, -0.4,...,0.5),
apply the three clustering algorithms with the parameter "number of
clusters" equal to 2,3,...,10 and register what number of clusters the
different evaluation methods chose. This was repeated 100 times and
the results were visualized as histograms. To provide an idea of how
the data sets looked with different amounts of separation, examples of
the generated data are visualized in Figure~\ref{facit}. Negative
separation value means that the intended clusters are overlapping
while a strictly positive separation value means that the intended
clusters are well separated. Also note that even though the clusters
have different spread and consist of different numbers of
observations, they all have more or less the same shape (they are all
more or less spherical clusters).

\graphicspath{{/home/mikael/Repos/Courses/msa220/Mini/Mini/Mini_1/}}
\begin{figure}[!h]
\begin{center}
\includegraphics[scale=0.5]{Facit.png}
\caption{Examples of generated clusters with different amounts of separation.}
\label{facit}
\end{center}
\end{figure}

The results of the experiments are presented as histograms in
Appendix~\ref{Plots Mini 1}. The histograms show that for all of the
clustering algorithms, all evaluation methods failed in finding the
correct number of clusters when the separation value was negative,
i.e.\ when the clusters were overlapping. This is not very
surprising. Both K-means and PAM will have difficulties to converge to
good cluster centers when the clusters are not well separated and the
same is true for the EM algorithm. Therefore the bad result might be
partly due to the failure of the clustering algorithms rather than the
evaluation methods. However, neither the average silhouette score
method or the Dunn index methods are designed to be used when the
clusters are not well separated. When clusters are overlapping, the
silhouette score will be small since the difference between the
average within cluster distances and the minimal average distance
between groups will be small. This makes it hard for the method to
differ between a good and a bad clustering. We also expect the method
to be a bit conservative when choosing the number of clusters, and
this was confirmed in this experiment since in most of the cases with
negative separation it chose fewer than 4 clusters. The Dunn index
method suffers from similar problems. When clusters are overlapping,
the Dunn ratio will be small even for a good clustering since the
minimal between cluster distance is smaller than the maximum within
cluster distance. Also, unlike the silhouette method, the Dunn method
seems to prefer a larger number of clusters when the clusters are not
well separated. This is not a surprise since when using many clusters,
the maximal within cluster distance will be smaller while the minimal
between clusters distance will remain about the same. The BIC
evaluation method for the EM algorithm chose fewer than 4 clusters
when the clusters were overlapping. Again, this is not a surprise
since the gain in likelihood obtained with more clusters will be small
compared to the increase in complexity.

In the cases where the clusters were well separated, all combinations
of evaluation method and clustering algorithm performed
well. K-means in combination with the silhouette method and the Dunn
method performed slightly worse than the other combinations. I think
this was due to poor performance of the K-means algorithm rather than
the evaluation methods. When I visualized the clustering, K-means had
a tendency to split a real cluster into two, i.e.\ one ore more of
the cluster means did not converge to a true cluster mean but rather
converged to a point between two clusters. Since PAM uses medoids
rather than mean points, it is more stable than K-means and did not
suffer from this problem, which explains the difference in the result
(at least in part). Also, the method used for generating clusters
compliments the EM algorithm since the clusters are normally
distributed and the EM algorithm assumes that the clusters comes from
a collection of normal distributions. This might explain why the combination
of the EM algorithm and the BIC method performed good when the
separation value was zero while the other methods failed. Even though
the EM algorithm most likely performed good when the number of
clusters was set to four and the separation was zero, the silhouette
method and the Dunn method were handicapped by the lack of separation,
while BIC managed to detect the good clustering sine it uses
likelihood rather than separation and denseness to evaluate the
result. This brings me to the flaws of the experiment setup.

The generated clusters were all ruffly spherical and both K-means and PAM are
designed to find these types of clusters. This makes the evaluation of
the evaluation methods a bit biased, since it is likely that the
result would have looked different if the clusters would have had
other shapes. Also, both the silhouette method the Dunn method were
designed to perform good on spherical clusters. Again, this adds bias
to the results. Another problem is the one mentioned above, namely
that the clusters were normally distributed and therefore fits the
model assumption of the EM algorithm perfectly. If one were to extend
this experiment, it would probably be a good idea to use data that is
not designed to fit the models and to use more diverge clustering
algorithms since both K-means and PAM are very similar. Even though
the experiment setup was biased, I still think that the  results
illustrates the difficulty in finding the correct number of clusters.

\newpage
\subsection{Mini 2}\label{Mini 2}
In this Mini analysis I compared the performance of different
classifiers and I also investigated two ensemble methods (bagging and
stacking) in order to see if any of them were able to improve on the
result obtained from using single classifiers. The original task
stated that we were supposed to compare the classifiers and ensembles
on 2-3 different data sets. However, I was working alone on this mini
and due to time limitations I was only able to analyse the result for
one data set since I put a lot of effort into feature engineering in
and pre-processing of the data. I hope that this deviation from the
instructions is acceptable.

The data I chose for this project was the Titanic data set from
Kaggle. It consists of 891 observations were each observation contains
information about a passenger on the Titanic. It has 12 columns, one
of which is a binary column (Survived) that states if the passenger
survived (1) or not (0). I chose this column to be my target column,
i.e.\ this is a binary classification task. The remaining 11 columns
contains information such as the sex of the passenger, his or her
passenger class (1,2,3) and so on. Some of the categorical columns
contained nearly as many levels as there were observations and could
not be used without some sort of pre-processing. For example, the
"Name" column contained the title and full name of the passenger. I
chose to extract the title from this column and drop the name of the
passenger since there were only a few titles in the data but every
name was unique. I treated some of the other categorical features
similarly (by for example extracting the ship section from the "Cabin"
column and discarding the unique cabin number), and an overview of all
the columns after pre-processing is presented in
Table~\ref{titanic features}. The "Age" column was numerical, but it
had missing values. I chose to impute the missing values with 0 and
add a dummy variable to indicate if the value was missing or not. In
retrospect, this was probably not the best thing to do since the
imputed values skewed the age-distribution. It would probably have
been better to either impute the median age or to use regression on
the other variables to impute the age.

\begin{table}
  \begin{center}
\begin{tabular}{ l l }
 Feature & Type \\ 
 \hline
 Pclass &   Numeric: 1,2,3\\
Title & Categories: Mr, Master, Mrs, Miss, Other\\
Sex & Categories: Male, Female\\
Age & Numeric\\
SibSp & Numeric: Number of siblings/spouses\\
Parch & Numeric: Number of parents/children\\
Fare & Numeric: Ticket price\\
Cabin & Categories: n, B, C, E, D, F, G, A\\
Embarked & Categories: C, S, Q\\
Age Missing & Dummy
\end{tabular}
\end{center}
\label{titanic features}
\end{table}

I used one hot encoding to represent the categorical features when
applying the various classifiers. For most of the classifiers, this is
a viable representation of categorical features. However, it is not a
good option when using logistic regression since the dummy
coefficients will be linearly dependent and thus cannot be used for
analysis. I did not think of this problem and if I would have tried to
interpret which variables that were significant for the probability of
survival it is likely that I would come to the wrong
conclusion. However, since I only used the model for prediction my
mistake should not affect the results.

Before applying any classifiers I split the data into a train- and a
test set. Since the original data set was small I did not want to
sacrifice to much data for testing and therefore I used a 90/10
split, resulting in 802 observations in the training data and 89
observations in the test data. I would have preferred to divide the
data into a train-, a validation- and a test set and use the
validation set to tune the hyper parameters in the different models.
But because of the small data set I instead chose to use 10-fold cross
validation for this purpose. The classifiers I compared in
this project were logistic regression, support vector classifiers
(SVC), CART, random forest, k-nearest neighbour classifiers (KNN), gradient
boosted classifiers (GBC), linear discriminant analysis (LDA), Naive
Bayes (NB), a neural network with three hidden layers (NN) and
quadratic discriminant analysis (QDA).

The results of the classifiers
are presented in Table~\ref{titanic classifiers result}. The results
were a bit surprising since there are three groups of classifiers that
got the exact same accuracy within the groups and there does not seem
to be any obvious similarities between the classifiers in the
groups. This rises the suspicion that many of the classifiers made the
exact same mistakes. Indeed, when I analyzed the miss-classified
observations for the different classifiers, most of them were
identical. What is more the set of observation that were
miss-classified by LR and SVC were a subset of the set of observations
that were miss-classified by most of the other classifiers. The fact
that LR and SVC (with a linear kernel) performed best indicates that there was a blurry
linear separation between the classes since both of these methods use
linear decision boundaries. I therefore suspect that the classes were
almost linearly separated, meaning that the separation was not perfect
but had some overlap. If this was the case it could explain why CART
and RF performed slightly worse since they would try to estimate the
linear "separation" using step functions. The same would be true for
GBC which, similarly to RF, uses several decision trees. LDA uses a
linear decision boundary as well. However it has in general more
trouble with finding the optimal boundary when the classes are not
perfectly separated than methods such as LR and SVC. NB and QDA uses
non-linear decision boundaries and both of these performed relatively
bad. However QDA requires more data than LDA and it is possible that
it was impaired by the small data set in this case. NN generally
requires a lot of training data as well in order to find a good
decision boundary and it is also possible that the architecture I used
was not suitable for the problem.

\begin{table}
\begin{center}
\begin{tabular}{ l | l l l }
 Classifier & Accuracy & P/R Survived & P/R Died \\ 
 \hline
 \hline
 LR & 0.876 & 0.86/0.91 & 0.90/0.83\\
SVC & 0.876 & 0.86/0.91 & 0.90/0.83\\
\hline
 CART & 0.865 & 0.84/0.91 & 0.89/0.81\\
RF & 0.865 & 0.84/0.91 & 0.89/0.81\\
KNN & 0.865 & 0.83/0.94 & 0.92/0.79\\
GBC & 0.865 & 0.82/0.96 & 0.94/0.76\\
LDA & 0.865 & 0.84/0.91 & 0.89/0.81\\
\hline
NB & 0.854 & 0.90/0.81 & 0.81/0.90\\
NN & 0.843 & 0.81/0.91 & 0.89/0.76\\
QDA & 0.820 & 0.82/0.85 & 0.82/0.79\\
\end{tabular}
\end{center}
\label{titanic classifiers result}
\end{table}

In Table~\ref{titanic classifiers result} the precision and recall for
the two classes are presented for all of the classifiers. Most of the
classifiers seems to have less precision when predicting survivors
than non-survivors while they have higher recall for the survivors
than the non-survivors. This suggests that the non-survivor class had
more spread than the survivor class. This is not very surprising since
some of the features levels, such as Sex: Female and PClass: 1,2, are
strong indicators of survival as can be seen in Figure~\ref{titanic
  sex pclass} in Appendix~\ref{Plots Mini 2}. 

I also tried stacking and bagging. Since the two best classifiers made
the exact same mistakes and since the mistakes done by these
classifiers were also made by the other classifiers, I did not expect
staking several classifiers to improve the accuracy at all. I was
right. I tried several different combinations of classifiers and they
all ended up with the same accuracy as one of the single classifiers
or a lower accuracy than the best classifiers in the
ensemble. The idea with stacking is that the errors or biases of
different classifiers are canceled out when using average
voting. However if all classifiers in the ensemble makes the same
mistakes there is nothing to gain from combining them.

I had higher hopes for bagging, since it is possible that using
slightly different training data for several classifiers of the same
type could improve the result. However the only classifier that
improved using bagging was CART as can be seen in Table~\ref{titanic
  bagging}. All other classifiers got the exact same accuracy when
bagged except for KNN which got a lower accuracy using bagging. This
suggests that the classifiers were fairly stable and not very
sensitive to small changes in the training data. KNN is sensitive to
small changes in the training data when using a small neighborhood (as
I did) and it is possible that the relatively good result obtained
from using a single KNN was due to luck. However I did not expect CART
to improve when bagged in this case, since CART and RF got the same
result when using a single classifier. I had expected this CART to be
stable, but it is clear that bagging was able to improve the result to
that obtained from a single LR and SVC. If my suspicion that the
classes were linearly separable except for some noise were correct, it is possible
that the decision boundary of the CART ensemble was a finer step
function that better approximated the linear separation.

\begin{table}
\begin{center}
\begin{tabular}{ l | l l}
 Classifier & Accuracy without bagging & Accuracy with bagging \\ 
 \hline
 CART & 0.865 & 0.876 \\
 KNN & 0.865 & 0.854\\
\end{tabular}
\end{center}
\label{titanic bagging}
\end{table}


\newpage
\subsection{Mini 3}\label{Mini 3}
In this mini analysis we compared different methods for dealing with
imbalanced classes in classification problems. Imbalanced classes can
pose a serious problem when doing classification. First of all, if the
data is heavily imbalanced, the number of training instances will be
small for the small class compared to the big class. This often
entails precision problems for the smaller class, i.e.\ the algorithm
might miss-classify observations from the smaller class more often
than observations from the bigger classes. This can be because small
number of training instances simply is not enough for the algorithm to
learn to distinguish between the classes. Also the loss caused by
miss-classifying observations from the small class can be relatively
small compared to the loss of miss-classifying observations from the
big classes.

The data set we chose to work with was a small diabetes data set consisting of
366 observations and 16 columns where one of the columns is a binary
variable that states if the patients have diabetes or not and the
remaining 15 columns contains information about the
patients. Most of these columns are numerical but a few of them are
categorical. The classes (sick/non-sick) were fairly imbalanced and only
15 percent of the observations were actually sick. We split the data
into a training and a test set containing 292 and 74 observations
respectively. That is, the training data was very small and since only
15 percent of the observations belonged to the sick class, there were
only a few sick observations in the training data. Since one of the members in
the group had worked with this data before, we knew that logistic
regression was one of the best classification algorithms for this
classification task. The goal with this analysis was to compare
different methods intended to improve the result of the
classifier. Many such methods exists, but we restricted our comparison
to three different up- or down-sampling methods, namely naive up- and
down-sampling and a more sophisticated up-sampling method called
\emph{smote}.

The naive up-sampling method is a method were duplicates are created
from some of the observations from the small class. The number of
duplicates is a hyper parameter that has to be set by the
user. Similarly, naive down-sampling randomly deletes a chosen
proportion of the majority class, thus balancing the data. The smote
method is similar to naive up-sampling, but instead of creating exact
duplicates of the observations, it uses an observations k-nearest
neighbours to create an artificial observation similar to the
observation and its neighbours. Since the data used for this
experiment is small, I expected naive up-sampling and smote to perform
a lot better than naive down-sampling. This is because down-sampling
does not use all the data available, and one can usually not afford
losing data when working with a small data set. 

Another aspect of the imbalanced class problem is the miss-leading
accuracy measure. If the data in a binary classification task has a
class frequency 90/10, the baseline accuracy is 90\% and if one only
considers the accuracy of a classifier one might be fooled into
believing that the classifier performs well even though in reality it
miss-classifies all observations from the smaller class. In our case,
we knew that the baseline accuracy was about 85\%. However we chose to
use three different performance measures to evaluate the methods,
namely accuracy, precision and recall. The precision and recall were
calculated with respect to the sick class. The reason for using
precision and recall was that it provides a good insight into the
performance of the classifier for the chosen class.

The experiment setup was to adjust the proportions of the two classes
using the different methods, train the classifier (logistic
regression) and evaluate the accuracy, precision and recall using the
test data. We did this for proportions of the small class ranging from
about 15\% to 90\%. The idea was to see how the classification result
would be affected by different levels of up- and
down-sampling. Ideally, we should have used a validation set to find the
best class proportions for each method before evaluating the
classifications with these proportions on a test set. However due to
the small data set we could not afford sacrificing data for a
validation set and we decided to compare the overall behaviour of the
different methods instead.

In Figure~\ref{mini 3 down-sampling}, Figure~\ref{mini 3 up-sampling}
and Figure~\ref{mini 3 smote} the results for the down-sampling,
up-sampling and smote are presented. The results are not very
surprising considering our small data set. For the down-sampling
method the accuracy drops relatively fast and so does the precision
for the sick class. However the recall increases as the proportion of
sick observations increases (or rather as the proportion of the
non-sick observation decreases). This means that more and more of the
sick observations are correctly classified, but at the cost of
lower precision, i.e.\ more and more of the non-sick observations are
miss-classified. This was to be expected. Removing instances from the
majority class when the training data is small will lead to more
miss-classified observations.

The results obtained from the naive up-sampling and smote are very
similar to each other. Both of them display a similar behaviour as
naive down-sampling, i.e.\ the accuracy and precision decreases and
the recall increases as the proportion of the small class
increases. However, even though the accuracy and precision decreases
they do so at a much lower rate than before while the recall increases
at about the same rate as for down-sampling. I expected this kind of
behaviour from these two methods. When the algorithm gets access to
more sick observations the loss for miss-classifying the observations
get more relevant and this forces the algorithm to try to learn how to
correctly classify the minor class. If the algorithm improves at this,
the recall will increase. At the same time it can no longer get away
with guessing "non-sick" when in doubt, and if the classes are not
fairly well separated the more non-sick observations will be
miss-classified. We see this as a decrease in precision. At the end of
the day, one has to decide if it is worth sacrificing precision for
recall. It all comes down to if it is more important to find most of
the sick observations or if one rather have a good precision.

The fact that naive up-sampling performed just as well as smote was
not a huge surprise since they are very similar. It seems that in this
case the algorithm did not benefit from having access to more diverse
sick training data. I imagine that smote might be more powerful if the
minor class contains more observations and if the minor class has more
spread within the class (while still being fairly well separated from
the other class). In this case, the simple naive approach worked well
and the amount of up-sampling used must be determined based on the
goal of the classifier. 

\graphicspath{{/home/mikael/Repos/Courses/msa220/Mini/Mini/Mini_3/}}
\begin{figure}[!htb]
\begin{center}
\includegraphics[scale=0.5]{LRDownsampling.png}
\caption{The accuracy, precision and recall at different amounts of down-sampling.}
\label{mini 3 down-sampling}
\end{center}
\end{figure}

\begin{figure}[!htb]
\begin{center}
\includegraphics[scale=0.5]{LRUpsample.png}
\caption{The accuracy, precision and recall at different amounts of up-sampling.}
\label{mini 3 up-sampling}
\end{center}
\end{figure}

\begin{figure}[!ht]
\begin{center}
\includegraphics[scale=0.5]{LRSmote.png}
\caption{The accuracy, precision and recall at different amounts of
  SMOTE up-sampling.}
\label{mini 3 smote}
\end{center}
\end{figure}

\newpage
\subsection{Mini 4}\label{Mini 4}

In this mini analysis we compared different methods for combining
clustering with different dimension reduction techniques. Clustering
algorithms use distances between observations to group similar
observations into clusters. When the number of dimensions grows larger,
the meaning of distances between observations begins to break down
(the curse of dimensionality). To work around the problem, different
methods can be utilized to reduce the dimensions before applying a
clustering algorithm. In this analysis we compared different
combinations of dimension reduction techniques and clustering
algorithms.

The data we used was a cancer data set with 801 observations and 20265
features. The features contained information about different genes and
the intended target variable was a categorical variable with 5
different cancer types. Since we were interested in clustering rather
than classification, we only used the cancer variable to evaluate the
clusters, i.e.\ we hoped that the clusters would represent different
types of cancer. However clustering is unsupervised and there were no
guarantees that the clusters would have any connection to cancer
types.

Before applying any dimension reduction techniques we tried to use
k-means and PAM on the original data. Applied the two algorithms with
different numbers of clusters and chose the number of clusters that
maximized the average silhouette score. The results are presented in
Appendix~\ref{Plots Mini 4}. In both cases the average silhouette
score suggested that we should use 6 clusters. However, there were
only 5 types of cancer and when looking at the clustering results with
6 clusters one of the cancer types were split into two clusters. Since
the average silhouette score for 5 and 6 clusters were similar, we
decided to use 5 clusters in order to be able to compare the results
more easily. Confusion matrices of the clusterings can be found in
Appendix~\ref{Plots Mini 4}. The results were surprising. Due to the
high dimensionality we did not expect k-means and PAM to perform as
well as they did. The fact they basically identified the cancer groups
perfectly in over 20 000 dimensions suggests that the groups were very
well separated in the original data space. Also, k-means and PAM are
designed to find spherical clusters, so their good result might
indicate that the clusters were multi-dimensional spheres. I have a
vague suspicion that the data were pre-processed. Maybe the data were
scaled within each cancer group, thus separating the clusters. In any
case, we continued with our analysis.

We tried two different dimension reduction techniques, PCA and
SOM. The reason for why we chose these particular because we wanted to
try both a global and a local method. In Figure~\ref{mini 4 pca} in
Appendix~\ref{Plots Mini 4} the explained variance for different
numbers of PCA variables is presented as well as the intended clusters
in the first three PCA variables. It is clear that the clusters are
fairly well separated in three dimensions and the elbow of the
explained variance-plot suggests that most of the variance is
explained using 100 PCA variables. Therefore, we reduced the data to
100 dimensions using PCA before using k-means and PAM. The results are
presented in Appendix~\ref{Plots Mini 4}. Both of the algorithms
performed well, meaning that the separation was more or less preserved
in a lower dimension. However, even though their results were good
they were not as good as they were when applied to the original data
set. A summary of the "accuracy" is presented in Table~\ref{mini 4
  summary} and it is clear that some of the separation was lost when
using PCA.

We also used a method called \emph{HDDC} (high dimensional data
clustering) which assumes that the clusters live in a subspace of the
data. It uses a k-means initialized EM-algorithm, i.e.\ it is a model
based approach. The idea is to find parameters of multivariate normal
distributions that fits the data in a subspace. This algorithm was
very slow and we were not able to run it on the original data
set. Instead we applied it to the PCA transformed data. This was
probably not optimal since even if the clusters lived in a subspace of
the original data there is no guarantee that they would do so in the
PCA transformed data. The results was not very impressive as
Table~\ref{mini 4 summary} suggests. Also, it only found two clusters
in the data where one of the clusters consisted of one of the cancer
types while the other cluster consisted of the remaining four
types. Either some of the information was lost when applying PCA or
the method was simply not appropriate for our data. It could also be
that the hyper parameters we used were sub-optimal.

When we used SOM to reduce the dimensions of the data, we used 64
(8$\times$8) nodes meaning that the reduction was fairly local. We
clustered the result from SOM using k-means, PAM and hierarchical
clustering. The results are presented in Table~\ref{mini 4
  summary}. Hierarchical clustering was clearly the best clustering
method out of the three and in Figure~\ref{mini 4 som} the clustering
of the SOM-map is presented. However the most interesting result is
that both k-means and PAM performed worse in combination with SOM
than they did in combination with PCA while hierarchical clustering
performed almost as good as k-means did on the original data. This
means that both the local and the global dimension reduction
techniques worked well in combination with different clustering
algorithms. Which combination to use is most likely case sensitive and
it is hard to draw any general conclusions from these results.

We also tried a subspace clustering method called
\emph{proclus}. Proclus is a method that works under the assumption
that the clusters live in a subspace of the data. It tries to find
these clusters by using medoids to find good cluster centers. It then
looks at the within cluster distances as a function of features and
tries to find the subspace in which the clusters live. The result is
presented in Appendix~\ref{Plots Mini 4}, and it was far from
impressing when viewed in relation to the other methods. However, the
algorithm was very slow, and there were a few hyper parameters to
tune. Due to time limitations we were not able to perform a grid
search to find the optimal parameters so the result might be
sub-optimal.  

\begin{table}[]
\centering
\scalebox{0.8}{
\begin{tabular}{|l|l|l|l|l|l|}
\hline
Dimension reduction/Method & kmeans & PAM    & hclust & EM     & ProClus \\ \hline
Original Data              & \cellcolor{green!80}99.3\% & 98.5\% & -      & -      & \cellcolor{red!80}72.7\%  \\ \hline
SOM                        & 87.5\% & 88.8\% & 99.1\% & -      & -       \\ \hline
PCA                        & 92.1\% & 90.8\% & -      & 93.8\% & -       \\ \hline
\end{tabular}
}
\label{mini 4 summary}
\caption{Summary of results}
\end{table}

The most surprising result from this mini analysis was that the best
clustering was obtained from applying k-means on the original data. I
did not expect this at all since k-means should be bad when the number
of dimensions is 20 000 due to the the curse of dimensionality. The
fact that it performed so well makes me suspicious that the data had
been pre-processed in some manner so that the cancer types were
very well separated and dense. This is just speculations though and
the main lesson from this analysis is that one has to try different
methods to find the one that is suitable for the data.

\subsection{Mini 5}\label{Mini 5}

In this mini analysis I worked in a group of three people. We worked
independently of each other and it seems like each of us were
investigating methods developed for different purposes. I will only
analyse my work and my approach here since it is not very meaningful
to compare methods that does not have the same goals.

I was analyzing the performance of an online classification method
called \emph{Adaptive-Size Hoeffding Tree} (ASHT), implemented in the
MOA package as the class \emph{OzaBagASHT}. The goal of online
classification is to train a classifier on a stream of data rather
than a data set, meaning that the classifier does not have access to
the full data set at ones. Instead, the training data comes as a
stream of data (or in chunks) and the classifier is only allowed to
see the data once, i.e.\ it cannot store the data it has already
seen. This is because there are many situations where storing massive
streams of data is simply not practically feasible. Also, the
classifier should be ready classify a new observation at any time,
allowing the training process to go on continuously while still being
ready to be used.

Another important aspect of data streams is that they
can change over time (concept drift), meaning that some aspect of the target variable
can change between different windows of the stream. Not all online
classification methods are designed to be able to handle concept
drift. ASHT is a method that can be used when concept drift is present
and the data I used for this analysis has a lot of drift. The data is
a electricity price data stream from Australia. The target column is a
binary column that states if the electricity price went up or down at
a specific point in time. It also contains 8 numeric features that I
used as training data.

A Hoeffding tree is basically a decision tree that grows larger and
larger as it observes more and more data. It uses the Hoeffding bound
to calculate how many observations it has to observe before making a
new split that satisfies the expected accuracy demands. The
ASHT method basically use an ensemble of such trees of different
sizes. The trees has a maximum allowed depth and when this depth is
reached the tree will start over from zero. The idea is that the
smaller trees in the ensemble will adapt quickly to drifts in the data
stream and that the bigger trees will be more stable and accurate over
time. The maximum depth limit is to ensure that the model adapts well
to concept drift since it will forget what it has learned after a
while. OzaBagASHT is an implementation of this method that is designed
to increase the diversity between the trees (using boosting) in order
to ensure a more stable classifier.

The experiment setup was to convert the data set into a data stream
that was fed into the classifier for training. At regular points
in the stream the classifier was tested on a test data set consisting
of the consecutive x observations in the stream. There were two
hyper-parameter to tune, namely the ensemble size and the depth of the
smallest tree in the ensemble. This was done manually in a pseudo-grid
search manner. The reason for why only the maximum depth of the
smallest tree has to be set is because the algorithm defines the
maximum depth of the other trees as $2s,~4s,..$ where $s$ is the
maximum depth of the smallest tree.

In order to visualize the concept drift in the stream I encoded the
"down" labels as -1 and the up variable as 1 and used a symmetric sliding window
of size 1312 to calculate the the mean y-value in each point. The
result is presented in Figure~\ref{mini 5 sliding window}. It is clear
from the figure that there is a lot of drift in this data and that it
changes quite fast. Also, the total sum is negative meaning that
there are more "Down" observations than "Up" observations.

\graphicspath{{/home/mikael/Repos/Courses/msa220/Mini/Mini/Mini_5/}}
\begin{figure}[!htb]
\begin{center}
\includegraphics[scale=0.5]{sliding_window.png}
\caption{The result of a sliding average over the target
  variable. "Down" was encoded as -1 and "Up" was encoded as 1.}
\label{mini 5 sliding window}
\end{center}
\end{figure}

The baseline accuracy for the data, i.e.\ the accuracy obtained from
only guessing "Down", is about 0.57, so any result above that is a
success. In the experiment I trained the classifier using 10 000
observation before testing it on the consecutive 500 and 10 000
observations. I then continued the training on the next 10 000
observations (including the ones which had been used for testing) and
tested again and so on. I was careful not to reset the classifier
after the testing sessions. I did this to simulate a continuous
training stream with regular testing sessions to keep track on the
performance. The results ate visualized in Figure~\ref{mini 5 result}.

\begin{figure}[!h]
\begin{minipage}{0.45\textwidth}
\includegraphics[width = \textwidth]{Pred_500.png}
\end{minipage}
\begin{minipage}{0.45\textwidth}
\includegraphics[width = \textwidth]{Pred_10K.png}
\end{minipage}
\label{mini 5 result}
\caption{\textbf{Left:} Train Size = 10 000, Test Size = 500,
  \textbf{Right:} Train Size = 10 000, Test Size = 10 000}
\end{figure}

The average accuracy was slightly higher in the experiment where I
used smaller test sets. This can of course be because data in these
test sets was easier to classify, but I am more inclined to think that the
difference was due to concept drift. As the figures suggests, the
class proportions changes rapidly and it is likely that the
classification  of observations was more accurate when the observation
came from a point in time close to the training window. Also, the
accuracy varied more between the test sets in the first experiments
than the latter. I suspect that the accuracy was more stable in the
second experiment because the trend changes in the data were average
out by the large training window.

I also tried using deeper trees. In the experiments above the maximum
depth of the first tree had depth 1. This means that the classifier
was set to adapt rapidly to changes in the data. Deeper first trees
lead to lower accuracy, meaning that the fast adaptation was needed
for this data. This is of course very data dependent and if the data
would have less drift it is likely that a classifier with deeper trees
would have been preferable. However, I used 10 trees in this ensemble
and this means that the largest tree were allowed to grow quite deep
(depth 512 if I understand the documentation correctly). It seems that
this was needed to obtain the optimal accuracy and it suggests that
even though the data changes quickly, the algorithm still benefited
from a long memory.

The take home message from this mini is that it is far from trivial to
build a good classifier with data that changes quickly. The ASHT
method is very intuitive and the fact that it did better on the
smaller testing sets on average indicates that even with data that
changes rapidly it still does a decent job at adapting to the
changes. However for this type of data I think that one can benefit
from methods developed specifically for fast changing data.  




\section{Question 2}\label{Question 2}

\section{Question 3}\label{Question 3}

\newpage
\appendix

\section{Plots Mini 1}\label{Plots Mini 1}

\graphicspath{{/home/mikael/Repos/Courses/msa220/Mini/Mini/Mini_1/new_plots/}}
\begin{figure}[!htb]
\begin{center}
\includegraphics[scale=0.5]{em_sil.png}
\caption{Histograms showing the optimal number of clusters for the EM
  algorithm according to the silhouette score method. The different
  histograms shows the results for artificially generated data with 4
  intended clusters and different separation coefficients.}
\label{em_sil_hists}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[scale=0.5]{em_dunn.png}
\caption{Histograms showing the optimal number of clusters for the EM
  algorithm according to the Dunn index method. The different
  histograms shows the results for artificially generated data with 4
  intended clusters and different separation coefficients.}
\label{em_dunn_hists}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[scale=0.5]{em_BIC.png}
\caption{Histograms showing the optimal number of clusters for the EM
  algorithm according to the BIC method. The different
  histograms shows the results for artificially generated data with 4
  intended clusters and different separation coefficients.}
\label{em_BIC_hists}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[scale=0.5]{km_sil.png}
\caption{Histograms showing the optimal number of clusters for the k-means
  algorithm according to the silhouette score method. The different
  histograms shows the results for artificially generated data with 4
  intended clusters and different separation coefficients.}
\label{km_sil_hists}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[scale=0.5]{km_dunn.png}
\caption{Histograms showing the optimal number of clusters for the k-means
  algorithm according to the Dunn index method. The different
  histograms shows the results for artificially generated data with 4
  intended clusters and different separation coefficients.}
\label{km_dunn_hists}
\end{center}
\end{figure}


\begin{figure}
\begin{center}
\includegraphics[scale=0.5]{pam_sil.png}
\caption{Histograms showing the optimal number of clusters for the PAM
  algorithm according to the silhouette score method. The different
  histograms shows the results for artificially generated data with 4
  intended clusters and different separation coefficients.}
\label{pam_sil_hists}
\end{center}
\end{figure}

\begin{figure}
\begin{center}
\includegraphics[scale=0.5]{pam_dunn.png}
\caption{Histograms showing the optimal number of clusters for the PAM
  algorithm according to the Dunn index method. The different
  histograms shows the results for artificially generated data with 4
  intended clusters and different separation coefficients.}
\label{pam_dunn_hists}
\end{center}
\end{figure}


\newpage
\section{Plots Mini 2}\label{Plots Mini 2}

\graphicspath{{/home/mikael/Repos/Courses/msa220/Mini/Mini/Mini_2/Titanic/}}
\begin{figure}[!htb]
\begin{center}
\includegraphics[scale=0.5]{sex_class.png}
\caption{The survival proportions for different sex/passenger class combinations.}
\label{titanic sex pclass}
\end{center}
\end{figure}

\newpage

\section{Plots Mini 4}\label{Plots Mini 4}

\graphicspath{{/home/mikael/Repos/Courses/msa220/Mini/Mini/Mini_4/TCGA-PANCAN-HiSeq-801x20531/}}
\begin{figure}[!htb]
\centering
\includegraphics[width=7cm]{O-KM-Silhouette.png}
\caption{The average silhouette scores for different numbers of
  clusters (K-means, Original)}
\end{figure}

\begin{table}[ht]
\centering
\resizebox{0.4\textwidth}{!}{%
\begin{tabular}{rrrrrr}
  \hline
 & BRCA & COAD & KIRC & LUAD & PRAD \\ 
  \hline
1 &   0 &   0 & 145 &   0 &   0 \\ 
  2 &   0 &   0 &   0 &   0 & 136 \\ 
  3 &   1 &   2 &   0 & 139 &   0 \\ 
  4 & 299 &   0 &   1 &   2 &   0 \\ 
  5 &   0 &  76 &   0 &   0 &   0 \\ 
   \hline
\end{tabular}
}
\caption{The result of k-means clustering on the original data.}
\end{table}

\begin{figure}
\centering
\includegraphics[width=7cm]{O-PAM-Silhouette.png}
\caption{The average silhouette scores for different numbers of
  clusters (PAM, Original)}
\end{figure}

\begin{table}[ht]
\centering
\resizebox{0.4\textwidth}{!}{%
\begin{tabular}{rrrrrr}
  \hline
 & BRCA & COAD & KIRC & LUAD & PRAD \\ 
  \hline
1 &   0 &   0 &   0 &   0 & 136 \\ 
  2 &   9 &   0 &   0 & 139 &   0 \\ 
  3 & 291 &   0 &   1 &   2 &   0 \\ 
  4 &   0 &   0 & 145 &   0 &   0 \\ 
  5 &   0 &  78 &   0 &   0 &   0 \\ 
   \hline
\end{tabular}
}
\caption{The result of PAM clustering on the original data.}
\end{table}

\begin{figure}
\centering
\begin{minipage}{0.45\textwidth}
\includegraphics[width=\textwidth]{PCA-Variance.png}
\end{minipage}
\begin{minipage}{0.45\textwidth}
\includegraphics[width=\textwidth]{PCA-3D.png}
\end{minipage}
\label{mini 4 pca}
\caption{Left: Explained variance for the different PCA
  variables. Right: The intended clusters in the first 3 PCA dimensions.}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=7cm]{PCA-KM-SW.png}
\caption{The average silhouette scores for different numbers of
  clusters (K-means, PCA)}
\end{figure}

\begin{table}[ht]
\centering
\resizebox{0.4\textwidth}{!}{%
\begin{tabular}{rrrrrr}
  \hline
 & BRCA & COAD & KIRC & LUAD & PRAD \\ 
  \hline
1 &  53 &   5 &   1 & 139 &   1 \\ 
  2 &   0 &   0 &   0 &   0 & 134 \\ 
  3 &   0 &   0 & 145 &   0 &   0 \\ 
  4 & 247 &   0 &   0 &   2 &   1 \\ 
  5 &   0 &  73 &   0 &   0 &   0 \\ 
   \hline
\end{tabular}
}
\caption{The result of k-means clustering on the PCA data.}
\end{table}


\begin{figure}
\centering
\includegraphics[width=7cm]{PCA-PAM-SW.png}
\caption{The average silhouette scores for different numbers of
  clusters (PAM, PCA)}
\end{figure}

\begin{table}[ht]
\centering
\resizebox{0.4\textwidth}{!}{%
\begin{tabular}{rrrrrr}
  \hline
 & BRCA & COAD & KIRC & LUAD & PRAD \\ 
  \hline
1 &   5 &   0 &   0 &   0 & 133 \\ 
  2 &  51 &   4 &   7 & 138 &   0 \\ 
  3 & 244 &   1 &   0 &   2 &   3 \\ 
  4 &   0 &   0 & 139 &   0 &   0 \\ 
  5 &   0 &  73 &   0 &   1 &   0 \\ 
   \hline
\end{tabular}
}
\caption{The result of PAM clustering on the PCA data.}
\end{table}

\begin{figure}[!ht]
\begin{center}
\includegraphics[scale=0.5]{sommap.jpeg}
\caption{The result of hierarchical clustering on the SOM-data.}
\label{mini 4 som}
\end{center}
\end{figure}

\newpage
\printbibliography



\end{document}
